## 2023: A significantly odd year for AI
I think CNN captured the mainstream sentiment around the tech world pretty succinctly when a few weeks ago they published the article ["It wasnâ€™t just you; things in the tech world got really weird this year"](https://www.cnn.com/2023/12/28/tech/tech-2023-weirdest-moments/index.html). We've seen the unsettling kerfuffle amongst the OpenAI board, the creepy hands imagined by Midjourney, and the spammy youtube videos clogging our watchlists. In my view it's definitely rooted in the mass adoption of Large Language Models and other forms of generative AI. No one really knows *how* these new technologies are going to affect the world. Not only are they still under-researched and relatively opaque, it takes a long time to understand how any new technology shapes the world!

It's kind of meta, but I think the area that's been affected the most by this is AI research. Anecdotally, it seemed like the mood of the AI world up until GPT 4's release was that language models - the deep learning architecture that LLMs like GPT-4 and LLAMA are a part of - are very interesting and powerful, but just one "method" of building and training AI models. I remember attending  Open Source Software Summit North America in Vancouver last May, and not seeing a single generative AI or LLM-related talk. Instead the focus was on PyTorch 2.0, microservice-based MLOps, and federated learning. Then only a few months later in July, I attended (and spoke at!) the Association of Computational Linguistics conference in Toronto - one could call it an LLM conference! Both the keynote speeches were on LLMs, and almost every talk I went to at least mentioned, if not actively worked with, some OpenAI product or other. It's the first time I heard the phrase "LLMs" too - to distinguish things like GPT4 and PaLM from "small" models like BERT and T5. Which is hilarious to me because up until now, those are what we'd call large, highly data-driven models! Of course, OSSNA had its own DevSecOps focus and ACL is most certainly the best place to talk about LLMs, but I thought it was interesting how quickly generative AI platforms like OpenAI have just *dominated* the AI and machine learning world.

I think the AI research space as a whole didn't expect language models to perform so reliably (in other words, not 100% completely unreliably) when they're trained and served at a large enough scale. In my view, it's kinda like a sudden left turn in machine learning research. There are now all these new questions to answer: how do we get the same level of performance out of something with less parameters? How do we reliably "look inside" big, opaque deep learning models? Do they fully understand language? It's affected areas from computer science and engineering to formal linguistics to developmental psychology!

So here are my favorite papers that I hope can capture some of the weirdness of machine learning and AI from the past year.

## 1. Machine translation from 5 sentence pairs
![The unreasonable effectiveness of few-shot learning for machine translation](/static/content/blog/ai-2023/garcia-2023.png)

(Garcia et al., 2023)

[This paper](https://arxiv.org/pdf/2302.01398.pdf) demonstrates an interesting shift in machine translation that's being facilitated by the increasing power and size of deep language models. For the past several decades, machine translation methods have focused on "alignment" - roughly speaking, designing algorithms whose "task" it is to match words in a source language and target language. This requires large amounts of "parallel" data - i.e., pairs of sentences, one in the "input" language, and the same sentence in the "output" language. Obviously, this isn't how most translation is done! It's had a big effect on the pace of machine translation --- it's much harder to get enough quality data because the bar is much higher compared to something broader like text classification.

Deep learning language models like BERT and GloVe are more flexible than the previous generation of statistical machine translation algorithms - they've enabled techniques like fine-tuning and backtranslation, which can increase the amount of non-strictly aligned language data that can be used. In other words, text in your input or output languages that don't strictly mean the same thing, or are at least on similar topics - including this extra data helps the model with generalizing and handling unexpected input.

But what if you had basically no aligned data at all? Just a large dataset in the input language, and a separate large dataset in the output language, and basically no knowledge of which parts of both datasets "mean the same thing?" Could you still train a deep learning model that can learn how to translate between the two languages? That's what Garcia et al. attempted to find out in this paper, and as I'm sure you guessed from the title, it worked amazingly well. They used an 8-billion parameter transformer model and trained it separately on English and German corpora. Think, a similar process to training a model like GPT3, but first on only English text, and then on only German text. Then, they supplied the model with only *five* direct English/German translations, and their model was able to perform just as well as the "baseline" models that were trained on parallel data.

This technique shows a lot of promise, particularly for training translation models in underserved and underrepresented languages across the world. On the other hand, the main breakthrough is the amount of computing power used - it takes a lot of money and GPU power to train a model that large, so it may be out of reach for the labs and researchers who specialize in low-resource natural language processing. Hopefully more research into cross-lingual transfer (can LLMs generalize across language?) or efficient deep learning training methods can close this gap in the future.

## 2. Do LLMs reason? Probably not really
![Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?](/static/content/blog/ai-2023/yiu-2023.png)

(Yiu et al., 2023)

[This paper](https://arxiv.org/pdf/2305.07666.pdf) covers the topics that came up in Alison Gopnik's keynote speech at ACL '23, titled "Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models." It was one of my favorite presentations at ACL and it's had a big influence on how I look at the current state of AI in the world today.

I think it's pretty clear that LLMs are not *alive* or *conscious*. They're definitely not human. They can create human-level text, and vision models like Dall-E or Midjourney can create realistic and interesting art. But are they *creative?* As a writer myself, I'm definitely in the "no" camp on that one. Even if they can generate entertaining or informative text, it's still an entirely probabilistic process that depends on the training data. I guess they lack that human, original "je ne sais quoi" that I think is essential to "creativity" when we think about writing that has artistic value. When you read something, don't you think *who* wrote it is kind of just as important as what's in it?

Anyway, I think this is related to what Yiu and Gopnik meant in the paper and talk when they called generative AI a "cultural technology" instead of an "agent." Something that can facilitate the spread of information to never-before-seen levels - think the internet, mass media, printing press, etc. As opposed to a "being" that makes its own decisions. And so, the questions that generative AI raises are less "will these things rise up against us?" and more "what will this mean for society as it currently functions? (or doesn't lol)"

The paper and talk also have a meta-analysis of recent psychology studies like Kosoy et al.'s 2022 paper [Towards Understanding How Machines Can Learn
Causal Overhypotheses](https://arxiv.org/pdf/2206.08353.pdf). The researchers gave a collection of state-of-the-art generative AI models a "blicket" study --- a procedure in childhood psychology that aims to study when "cause-and-effect" and "rule-based" reasoning develops in children. Roughly speaking, they described to their subjects - GPT-2 and -3 and others - a little toy that consists of a platform and blocks of various shapes and colors. The task is to determine if there's some kind of pattern to which kinds of shapes, when placed on the platform, cause it to light up. The LLMs were either unable to find the patterns, or required hundreds of tries in order to make correct predictions. For reference, a typical preschooler can pass a blicket test after a handful of tries.

![An illustration of a blicket study. A purple triangle lights up a platform when placed on it. A yellow cylinder doesn't. The combination of the yellow cylinder and the purple triangle makes it light up: a disjunctive rule. A blue triangle doesn't light the platform up. Neither does an orange circle. But both the orange circle and blue triangle make it light up. This is a conjunctive rule. Under the illustration: Figure 2: a simplified representation of the examples that children see in the *given hypothesis* training phase.](/static/content/blog/ai-2023/blicket.png)

(Kosoy et al., 2022)

What this paper shows is that generative AI models can create highly technical text and flashy art, but they struggle with "higher-level" reasoning - things like creating novel inferences or understanding temporal relations, which children typically learn in their first few years of life. It has interesting implications for the relation of language, reasoning, and consciousness in the brain. It also shows the limitations of our current AI systems - they *do* have them! And plenty of them!

## 3. Debugging Stable Diffusion
![What the DAAM: Interpreting Stable Diffusion Using Cross Attention](/static/content/blog/ai-2023/tang-2023.png)

(Tang et al., 2023)

Oh look, it's my favorite genre of machine learning paper! System demo with a funny backronym name. [DAAM](https://aclanthology.org/2023.acl-long.310.pdf) is a new method for interpreting Stable Diffusion inference - in other words, determining the "inner workings" of a deep text-to-image model.

For reference, Stable Diffusion is an open source image generation model. It takes in a text prompt and generates an image based on it - one of those infamous "AI art" systems of late. Potent copyright issues aside, it's a pretty fascinating example of "multimodal" AI - deep learning systems that work across traditionally quite separate machine learning disciplines like text and vision and tabular data.

The idea behind DAAM is the question "what parts of the output image are related to which words in the input prompt?" The algorithm essentially can make "heatmaps" based on an input prompt, a specific word in the prompt, and a Stable Diffusion-generated image.

![An illustration of DAAM in action. The caption says "Figure 1: the original synthesized image and three DAAM maps for 'monkey,' 'hat,' and 'walking,' from the prompt 'monkey with hat walking.' We see four images of a monkey in a hat. The first is the original. The next has a red heatmap blob around the monkey's face. The next has a blob around the monkey's hat. The final has a blob around the monkey's legs.](/static/content/blog/ai-2023/daam-monkey.png)

(Tang et al., 2023)

I really like this paper because it demonstrates the potential of deep learning interpretability research. People think of deep learning models as if they're "black boxes" - the training algorithms themselves are mathematically sound and well-studied, but the created models are opaque - they're essentially big bundles of data representing weights and biases, and it's almost impossible to know what exactly is represented. It's a huge issue when actually deploying deep learning models. Would you rather have a process that works *most* of the time or *all* of the time? And if you're stuck with the former, wouldn't you like some kind of objective way to measure and reason about what *most of the time* even means?

DAAM shows that it *is* possible to shed light on how models like Stable Diffusion behave, and how they process linguistic and visual data. It's one of the most important and unfortunately underrated subfields of deep learning research, in my view!

## 4. An absurd amount of new crystals
![Scaling deep learning for materials discovery](/static/content/blog/ai-2023/merchant-2023.png)

(Merchant et al., 2023)

So obviously a [paper published in *Nature*](https://www.nature.com/articles/s41586-023-06735-9) is going to be interesting in some way, and this is a study that appeared in a lot of mainstream news sources. I think the sheer scale of this project is noteworthy - using a Graph Neural Network, a team at Google DeepMind have discovered 2.2 *million* potentially stable inorganic crystals.

Now I'm no chemist, so take my explanation with a chemically appropriate grain of salt. But the idea is, inorganic crystals on the atomic level are networks of atoms connected by bonds. A crystal with a low energy level is stable and less likely to break apart, and the higher the energy level is, the less stable it is.

The "search space" of this problem - all the possible combinations of elements and bonds that could make a new inorganic compound - is gigantic. Large enough that you certainly couldn't even list out all the potential combinations, let alone record their energy levels. So the model introduced in this paper, called Graph Network for Matrerials Exploration, used a training technique called active learning. In addition to predicting an input crystal's energy, it also learned how to ask for the *next* input crystal to test.

This shows the sheer number of applications for deep learning. Graph Neural Networks come up in natural language processing too - the same architecture that helped discover new materials is also used for tasks like commonsense reasoning - in other words, methods for preventing "hallucinations" in LLMs. In other other words, ChatGPT saying stuff wrong and pretending it isn't.

I've always loved how cross-discipline machine learning research can be. However, domain knowledge is still an essential skill. More and more people are becoming experts on deep learning and AI, but that doesn't mean they're experts on other things!

## 5. That one time a 40-year-old zip algorithm outranked BERT
!["Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors](/static/content/blog/ai-2023/jiang-2023.png)

(Jiang et al., 2023)

I've saved my [absolute favorite paper](https://aclanthology.org/2023.findings-acl.426.pdf) for last - it's lived rent-free in my head ever since it was published. Here's the background: "text classification" is a pretty straightforward task when you compare it to things like text-to-image generation or new materials discovery. The goal when you boil it down really is to just assign a number label to a blob of text. For example, there's the case of "topic modelling" - which of a finite number of topics - say business, technology, or sports - does an article "belong to?"

Despite the relative simplicity of the problem, it's still a difficult task to get right. The current best option is a deep classifier - basically, hook a classifier network up to the final nodes of a language model like BERT or one of the GPTs. This works reasonably well - the baseline prediction accuracy of a range of state-of-the-art deep classifiers in this study was around 90% for the topic modelling task. The downside, however, is that they require a lot of data to train and host. Or, you have to buy in to the OpenAI ecosystem.

But then this study came out of ACL '23 - it's been dubbed the "Gzip paper" because of their novel classifier algorithm. It's basically just gzip, a ubiquitous file compression algorithm. Anyone who uses a unix-based OS like Linux or OSX has probably gzipp'd plenty of data without thinking of it. And they found that with basically just ten lines of Python code, their algorithm *ties* with the best deep classifiers!

The success actually raises some interesting theoretical questions about the relationship between compression and probability. It's honestly something I'd never thought to think of before, if you know what I mean! It also shows that there's still a lot to discover in the world of machine learning and AI research, and that the biggest, most powerful, most expensive approaches aren't always the best!

## Conclusion
The past year was, if anything, strange. I don't think anyone truly expected LLMs and generative AI to take off in the way that they did, and now we're going into 2024 facing a bunch of very difficult questions. My opinion on all of this is a resounding "oh geez this is all so complicated and I just wanna wait and see how this all plays out."

Nonetheless, I really do enjoy writing and research and learning what I can about these emerging technologies. It does suck that it's now possible to just prompt a third-party LLM for an article, upload it to the internet, and probably get far more recognition than anything I'd write by actually putting the work in. But, the act of writing the darn thing is the whole point, to me at least! And I'm glad you took the time to read all of this.

This past year has made me really lean in to my "creative skills" - things like communication and project management and inspiration. I've also seen the Computational Linguistics and Natural Language Processing fields go from a very technical discipline focused on new algorithm design to one that's more open to qualitiative analysis and overlap with the humanities and social science.

So my prediction for 2024 is it's gonna be pretty similar to 2023 but also perhaps slightly different. If neural networks can get away with this kind of thinking, perhaps I'll give it a go too!